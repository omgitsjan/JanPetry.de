---
title: "[DE] Lokales LLM mit GPT4All"
description: √úber die Feiertage habe ich etwas mit lokalen LLMs und GPT4All
  herumgetestet um zu sehen was heute schon m√∂glich ist in der welt der
  K√ºnsltlichen Intelligenz.
image: /blog/gpt.webp
minRead: 6
date: 2023-12-27
author:
  - name: Jan Petry
    to: https://github.com/omgitsjan
    avatar:
      src: https://avatars.githubusercontent.com/u/42674570
      alt: Jan Petry
---

## ü§ñ Experiment √ºber die Feiertage: Lokales LLM mit GPT4All ü§ñ

W√§hrend der Feiertage widmete ich mich einem faszinierenden Experiment: Ich betrieb ein Large Language Model (LLM) lokal auf meinem System, unterst√ºtzt von meiner GPU. Ziel war es, die Machbarkeit und Effizienz eines solchen Unterfangens zu erkunden.

## Start mit GPT4All

Das Herzst√ºck des Projekts war GPT4All, ein beeindruckendes Open-Source-Projekt mit √ºber 50k Sternen auf GitHub (<https://lnkd.in/eKzKwiQg>). Dank der benutzerfreundlichen Installation und einem Interface, das dem von OpenAI, Microsoft und Google √§hnelt, war der Einstieg ein Kinderspiel. Besonders praktisch war die M√∂glichkeit, aus einer Vielzahl von AI-Modellen zu w√§hlen, entweder √ºber den integrierten Browser oder durch direkten Download.

## Modellauswahl: Mistra OpenOrca

Ich entschied mich f√ºr Mistra OpenOrca, ein beeindruckendes, kostenloses Modell, das sich ideal f√ºr kommerzielle Anwendungen eignet. Trotz seiner geringeren Komplexit√§t im Vergleich zu GPT-4 zeigte es beeindruckende Leistungen.

### Features und Sprachflexibilit√§t

Ein Highlight war die Funktion ‚ÄûLocal Document Collections‚Äú, die es mir erm√∂glichte, dem Modell Zugriff auf pers√∂nliche Dokumente ‚Äì zum Beispiel mein Tagebuch ‚Äì zu gew√§hren. Hierbei traten allerdings noch einige Herausforderungen auf. Zudem verarbeitete das Modell Anfragen auf Deutsch, antwortete jedoch gelegentlich auf Englisch.

### Performance-Details

Das Modell lief auf meiner Radeon-GPU √ºberraschend effizient, mit einer Spitzenauslastung von 78 %. Mit einem RAM-Bedarf von rund 8 GB und einer Modellgr√∂√üe von etwa 4 GB war es gut handhabbar. Die beeindruckenden 7 Milliarden Parameter des Modells erm√∂glichten es, komplexe Muster in den Daten zu erkennen und mit einer beachtlichen sprachlichen Vielfalt zu reagieren. Es ist bemerkenswert, dass solch fortschrittliche Modelle nun lokal und mit eigener Hardware betrieben werden k√∂nnen.

## Abschlie√üende Gedanken

Dieses Experiment war nicht nur eine spannende Herausforderung, sondern hat auch das enorme Potenzial lokaler LLMs aufgezeigt. Ich freue mich darauf, diese Technologie weiter zu erkunden und bin gespannt auf eure Gedanken und Erfahrungen in diesem Bereich. Lasst uns gemeinsam die M√∂glichkeiten dieser faszinierenden Technologie erforschen! Unten finden Sie einige Screenshots meiner Erfahrungen.

Guten Rutsch ins neue Jahr! üéÜ

- _Jan_
